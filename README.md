# text-autocomplete-project

### Документация проекта
Этот проект реализует два подхода к задаче предсказания следующего слова в тексте:
- LSTM-модель на основе PyTorch
- Трансформерную модель на базе distilgpt2 

---

### Структура проекта
#### Схема
```
text-autocomplete-project/
├── data/                                   # датасеты и словари (информация по версиям full/final/test ниже)
│   ├── raw_data.csv                        # скачанный датасет
│   ├── vocab.pkl                           # словарь
│   ├── train_dataset.pt                    # тренировочная выборка
│   ├── test_dataset.pt                     # тестовая выборка
│   └── val_dataset.pt                      # валидационная выборка
│
├── src/                                    # код проекта
│   ├── data_utils.py                       # Обработка датасета
|   ├── token_dataset.py                    # код с torch Dataset'ом 
│   ├── lstm_model.py                       # код lstm модели
|   ├── eval_lstm.py                        # замер метрик lstm модели
|   ├── lstm_train.py                       # код обучения модели
|   └── eval_transformer_pipeline.py        # код с запуском и замером качества трансформера
│
├── models/                                 # веса обученных моделей (описание ниже)
|
├── solution.ipynb                   # ноутбук с решением
└── requirements.txt                 # зависимости проекта
```

---

#### Итоговые артефакты 
(остальные (_full или _test) использовались в экспериментах, о полному датасету обучение вообще не идёт или происходит очень долго, проверяла на 4060 Ti и 4080 - ждать 6+ часов обучения на твитах не очень хочется, и адекватные гиперпараметры для обучения заранее никто не пишет)
- Словарь: `data/vocab_final.pkl`
- Датасеты: `data/train_dataset_final.pt`, `val_dataset_final.pt`, `test_dataset_final.pt`
- Модель: `models/best_lstm_model_final.pt`

---

### Основные модули и классы

#### 1. `AutoregressiveTokenDataset` (в `src/token_dataset.py`)
**Назначение**: Создаёт обучающие пары `(контекст, следующее_слово)` из текстов.

**Поля**:
- `texts`: список строк (текстов)
- `word_to_idx`: словарь для преобразования слов в индексы
- `seq_len`: длина контекста (по умолчанию 10)

**Методы**:
- `__init__(texts, word_to_idx, seq_len)`: инициализация
- `__len__()`: возвращает количество обучающих примеров
- `__getitem__(idx)`: возвращает тензоры `(input_ids, target_id)`

> Использует паддинг (`<PAD>`) для коротких контекстов.

---

#### 2. `LSTMTokenizerModel` (в `src/lstm_model.py`)
**Назначение**: Простая LSTM-модель для предсказания следующего слова.

**Архитектура**:
- Слой эмбеддингов
- Многослойный LSTM
- Полносвязный слой на выходе

**Параметры**:
- `vocab_size`: размер словаря
- `embed_dim=128`: размер эмбеддингов
- `hidden_dim=256`: скрытое состояние LSTM
- `num_layers=2`: количество слоёв LSTM

**Методы**:
- `forward(x)`: прямой проход, возвращает логиты для следующего слова

---

#### 3. `TransformerModel` (в `eval_transformer_pipeline.py`)
**Назначение**: Обёртка вокруг `distilgpt2` для генерации и оценки.

**Использует**:
- `AutoModelForCausalLM` и `AutoTokenizer` от Hugging Face
- `pipeline("text-generation")`

**Методы**:
- `__init__(model_name="distilgpt2")`: загрузка модели и токенизатора
- `load_tweets_from_csv(file_path)`: загрузка текстов
- `clean_tweet(text)`: очистка твита (удаление ссылок, упоминаний, нормализация)
- `prepare_data(file_path)`: подготовка валидационных/тестовых данных
- `validate_model(val_texts)`: оценка по ROUGE-L на валидации
- `generate_texts(train_texts)`: генерация текстов по промптам

---

#### Ключевые функции

##### `prepare_data(...)`
**Модуль**: `code.txt`  
**Назначение**: Загрузка, очистка и разбиение данных.

**Этапы**:
1. Загрузка текстов из файла
2. Очистка: удаление URL, `@`, `#`, HTML-сущностей, нормализация сокращений
3. Фильтрация по длине (по умолчанию 4–16 слов)
4. Разделение на train/val/test
5. Построение словаря (`word_to_idx`, `idx_to_word`)

---

##### `create_data_loaders(...)`
Создаёт `DataLoader`'ы для train/val/test с использованием `AutoregressiveTokenDataset`.

---

##### `evaluate_with_rouge(...)`
Оценивает модель по:
- Loss (CrossEntropy)
- Accuracy
- Perplexity
- ROUGE-L (на уровне одного слова)

---

### `evaluate_final(...)`
Финальная оценка на тестовой выборке + анализ генерации:
- Примеры сгенерированных текстов
- Средняя длина, уникальность слов
- Топ-5 слов в генерации
- Количество параметров

---

##### `generate(...)`
**Назначение**: Генерация текста с помощью LSTM-модели.

**Параметры**:
- `seed_words`: начальный контекст
- `temperature`: контролирует случайность
- `top_k`: ограничение по топ-k токенам
- Останавливается на `.`, `!`, `?` или при превышении длины

---

##### `train()`
Основной цикл обучения:
- Использует `AdamW` и `ReduceLROnPlateau`
- Early stopping (после 7 эпох без улучшения)
- Логирование прогресса на фиксированном примере

---

#### Метрики

| Метрика       | Описание |
|---------------|---------|
| **Accuracy**  | Доля правильно предсказанных следующих слов |
| **Perplexity** | `exp(loss)` — чем ниже, тем лучше |
| **ROUGE-L**   | Сходство между истинным и предсказанным словом (по F1) |

---

### Запуск проекта
#### Зависимости
- нужная версия не ставится через обычный pip install, убрала из requirements.txt
```
pip install torch==2.5.1 torchvision==0.20.1 torchaudio==2.5.1 --index-url https://download.pytorch.org/whl/cu121
```
- для остального подойдёт 
```
pip install -r requirements.txt
```

