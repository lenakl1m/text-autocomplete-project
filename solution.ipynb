{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec40bd00",
   "metadata": {},
   "source": [
    "## Этап 1. Сбор и подготовка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "473da1fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "всего строк в файле: 1600498\n",
      "оставлено после сэмплирования: 320099 строк\n",
      "\n",
      "примеры очистки:\n",
      "до:  @bkajino That awesome was for the walk, not the blisters\n",
      "после: that awesome was for the walk not the blisters\n",
      "\n",
      "до:  @trent_reznor @mariqueen it makes me sad to see all the bs ppl are writing about and to you two.  I don't know why they can't let you be..\n",
      "после: it makes me sad to see all the bs ppl are writing about and to you two i don't know why they can't let you be\n",
      "\n",
      "до:  i give up on them !\n",
      "после: i give up on them\n",
      "\n",
      "до:  @jeanettejoy  Well said.  Never follow someone because of their name, follow because they have something worth listening too\n",
      "после: well said never follow someone because of their name follow because they have something worth listening too\n",
      "\n",
      "до:  Working on 12 with @sherrisnack\n",
      "после: working on 12 with\n",
      "\n",
      "после фильтрации по длине [4–16]: 198079 текстов\n",
      "средняя длина: 9.6 слов\n",
      "мин: 4, макс: 16\n",
      "train: 158463\n",
      "val:   19808\n",
      "test:  19808\n",
      "размер словаря: 61013\n",
      "примеры: 'love' -> 4802, 'the' -> 25945\n",
      "размер train_dataset: 1363329 пар (контекст → следующий токен)\n",
      "пример из train_dataset[0]: input=tensor([    0,     0,     0,     0,     0,     0,     0,     0,     0, 37125]), target=37905\n",
      "x_batch.shape: torch.Size([128, 10])  # [b, seq_len]\n",
      "y_batch.shape: torch.Size([128])  # [b]\n",
      "датасет готов, словарь сохранён\n"
     ]
    }
   ],
   "source": [
    "from src.data_utils import (\n",
    "    prepare_data,\n",
    "    create_data_loaders,\n",
    "    save_artifacts\n",
    ")\n",
    "\n",
    "# запуск подготовки\n",
    "train_texts, val_texts, test_texts, word_to_idx, idx_to_word, vocab_size, seq_len = prepare_data(\n",
    "    file_path='data/raw_data.csv',\n",
    "    sample_ratio=0.2,\n",
    "    min_len=4,\n",
    "    max_len=16,\n",
    "    seq_len=10,\n",
    "    random_seed=42\n",
    ")\n",
    "\n",
    "# даталоадеры\n",
    "train_loader, val_loader, test_loader, train_dataset, val_dataset, test_dataset = create_data_loaders(\n",
    "    train_texts, val_texts, test_texts,\n",
    "    word_to_idx=word_to_idx,\n",
    "    seq_len=seq_len,\n",
    "    batch_size=128\n",
    ")\n",
    "\n",
    "# сохраняем\n",
    "save_artifacts(word_to_idx, idx_to_word, vocab_size, seq_len, train_dataset, val_dataset, test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20ed40b",
   "metadata": {},
   "source": [
    "## Этап 2. Объявление модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2e43ab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMTokenizerModel(\n",
      "  (embedding): Embedding(61013, 128, padding_idx=0)\n",
      "  (lstm): LSTM(128, 256, num_layers=2, batch_first=True)\n",
      "  (fc): Linear(in_features=256, out_features=61013, bias=True)\n",
      ")\n",
      "размер словаря: 61013\n",
      "модель создана на устройстве: cuda\n",
      "количество параметров: 24,411,605\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import torch\n",
    "from src.lstm_model import LSTMTokenizerModel\n",
    "\n",
    "# словарь\n",
    "with open('data/vocab_test.pkl', 'rb') as f:\n",
    "    vocab_data = pickle.load(f)\n",
    "\n",
    "# данные из словаря\n",
    "word_to_idx = vocab_data['word_to_idx']\n",
    "idx_to_word = vocab_data['idx_to_word']\n",
    "\n",
    "# параметры модели\n",
    "vocab_size = vocab_data['vocab_size']\n",
    "embed_dim = 128\n",
    "hidden_dim = 256\n",
    "num_layers = 2\n",
    "seq_len = vocab_data['seq_len'] \n",
    "\n",
    "# экземпляр модели\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = LSTMTokenizerModel(\n",
    "    vocab_size=vocab_size,\n",
    "    embed_dim=embed_dim,\n",
    "    hidden_dim=hidden_dim,\n",
    "    num_layers=num_layers\n",
    ").to(device)\n",
    "\n",
    "print(model)\n",
    "print(f\"размер словаря: {vocab_size}\")\n",
    "print(f\"модель создана на устройстве: {device}\")\n",
    "print(f\"количество параметров: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6bc077",
   "metadata": {},
   "source": [
    "## Этап 3. Тренировка модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd7842c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "словарь загружен: 61013 токенов\n",
      "пример: 'the' -> 59374\n",
      "длина контекста (seq_len): 10\n",
      "размер обучающей выборки: 1363329\n",
      "пример входа: (tensor([    0,     0,     0,     0,     0,     0,     0,     0,     0, 42614]), tensor(51857)) -> you\n",
      "используем устройство: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Лена\\text-autocomplete-project\\venv\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== дебаг: проверка данных ===\n",
      "пример x_batch[0]: [0, 0, 0, 0, 29929, 59374, 48525, 17313, 34665, 35502]\n",
      "y_batch.min(): 686\n",
      "y_batch.max(): 60411\n",
      "✅ forward и loss работают\n",
      "============================================================\n",
      "фиксированный пример для сравнения каждую эпоху\n",
      "============================================================\n",
      "контекст: thanks\n",
      "реальное продолжение: for\n",
      "→ ожидаем: thanks for\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1:  11%|█         | 1132/10652 [00:19<02:40, 59.34it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meval_lstm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m evaluate_final\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# запуск обучения\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m model, word_to_idx, idx_to_word, seq_len, device, test_loader \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# финальная оценка\u001b[39;00m\n\u001b[0;32m     14\u001b[0m evaluate_final(model, test_loader, word_to_idx, idx_to_word, seq_len, device)\n",
      "File \u001b[1;32mc:\\Users\\Лена\\text-autocomplete-project\\src\\lstm_train.py:110\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    108\u001b[0m logits \u001b[38;5;241m=\u001b[39m model(x_batch)\n\u001b[0;32m    109\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(logits, y_batch)\n\u001b[1;32m--> 110\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    111\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    112\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\Лена\\text-autocomplete-project\\venv\\Lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Лена\\text-autocomplete-project\\venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Лена\\text-autocomplete-project\\venv\\Lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "from src.lstm_model import generate\n",
    "from src.lstm_train import train \n",
    "from src.eval_lstm import evaluate_final\n",
    "\n",
    "# запуск обучения\n",
    "model, word_to_idx, idx_to_word, seq_len, device, test_loader = train()\n",
    "\n",
    "# финальная оценка\n",
    "evaluate_final(model, test_loader, word_to_idx, idx_to_word, seq_len, device)\n",
    "\n",
    "# примеры генерации\n",
    "print('\\nгенерация:')\n",
    "print('i ->', generate(model, 'i', word_to_idx, idx_to_word, seq_len, temperature=0.8, top_k=10))\n",
    "print('i love ->', generate(model, 'i love', word_to_idx, idx_to_word, seq_len, temperature=0.8, top_k=10))\n",
    "print('today ->', generate(model, 'today', word_to_idx, idx_to_word, seq_len, temperature=0.8, top_k=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c524914c",
   "metadata": {},
   "source": [
    "## Этап 4. Использование предобученного трансформера distilgpt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91bdd40e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "загружаем модель distilgpt2\n",
      "используется GPU\n",
      "загружаем и подготавливаем данные\n",
      "загружено 1600498 твитов\n",
      "очищено текстов: 1600498\n",
      "отфильтровано текстов: 1251724\n",
      "валидационная выборка: 150 текстов\n",
      "----------------------------------------\n",
      "валидация модели\n",
      "----------------------------------------\n",
      "выполняем оценку модели на валидационной выборке\n",
      "\n",
      "----------------------------------------\n",
      "результаты валидации\n",
      "----------------------------------------\n",
      "метрика: ROUGE-L (точность предсказания следующего слова)\n",
      "количество оценок: 100\n",
      "средний ROUGE-L: 0.0867 ± 0.2774\n",
      "медиана ROUGE-L: 0.0000\n",
      "\n",
      "лучшие примеры предсказаний:\n",
      "----------------------------------------\n",
      "пример 1:\n",
      "  контекст:    'i hope i can make'\n",
      "  истинное:    'it'\n",
      "  предсказанное: 'it'\n",
      "  ROUGE-L:     1.0000\n",
      "\n",
      "пример 2:\n",
      "  контекст:    'what tragedy and disaster in'\n",
      "  истинное:    'the'\n",
      "  предсказанное: 'the'\n",
      "  ROUGE-L:     1.0000\n",
      "\n",
      "пример 3:\n",
      "  контекст:    'yes yes still trying to'\n",
      "  истинное:    'find'\n",
      "  предсказанное: 'find'\n",
      "  ROUGE-L:     1.0000\n",
      "\n",
      "пример 4:\n",
      "  контекст:    'i know right i dunno'\n",
      "  истинное:    'what'\n",
      "  предсказанное: 'what'\n",
      "  ROUGE-L:     1.0000\n",
      "\n",
      "пример 5:\n",
      "  контекст:    'i miss him can't wait'\n",
      "  истинное:    'to'\n",
      "  предсказанное: 'to'\n",
      "  ROUGE-L:     1.0000\n",
      "\n",
      "----------------------------------------\n",
      "генерация текстов\n",
      "----------------------------------------\n",
      "генерируем тексты на основе промптов\n",
      "----------------------------------------\n",
      "промпт 1:\n",
      "  вход:  i dived many times for the ball managed to save 50 the\n",
      "  выход: game.\n",
      "\n",
      "  полностью: i dived many times for the ball managed to save 50 the game.\n",
      "\n",
      "----------------------------------------\n",
      "промпт 2:\n",
      "  вход:  i just re pierced\n",
      "  выход: , and I can't wait to see him again.\n",
      "\n",
      "  полностью: i just re pierced, and I can't wait to see him again.\n",
      "\n",
      "----------------------------------------\n",
      "промпт 3:\n",
      "  вход:  i couldn't bear to watch it and i thought the\n",
      "  выход: other day we had a blast.\n",
      "\n",
      "  полностью: i couldn't bear to watch it and i thought the other day we had a blast.\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "генерация завершена\n"
     ]
    }
   ],
   "source": [
    "from src.eval_transformer_pipeline import TransformerModel\n",
    "\n",
    "# экземпляр модели\n",
    "model = TransformerModel()\n",
    "\n",
    "# данные\n",
    "test_texts, val_texts = model.prepare_data('data/raw_data.csv')\n",
    "\n",
    "# валидация модели\n",
    "avg_rouge, examples = model.validate_model(val_texts)\n",
    "\n",
    "# генерируем тексты\n",
    "generation_results = model.generate_texts(test_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fcb782",
   "metadata": {},
   "source": [
    "## Этап 5. Формулирование выводов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8df00da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
