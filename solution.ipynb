{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec40bd00",
   "metadata": {},
   "source": [
    "## Этап 1. Сбор и подготовка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "473da1fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "загружаем и подготавливаем данные\n",
      "----------------------------------------\n",
      "загружено 1600498 строк из файла\n",
      "всего строк в файле: 1600498\n",
      "оставлено после сэмплирования: 320099 строк\n",
      "очищено текстов: 320099\n",
      "----------------------------------------\n",
      "\n",
      "примеры очистки:\n",
      "до:  @bkajino That awesome was for the walk, not the blisters\n",
      "после: that awesome was for the walk not the blisters\n",
      "\n",
      "----------------------------------------\n",
      "до:  @trent_reznor @mariqueen it makes me sad to see all the bs ppl are writing about and to you two.  I don't know why they can't let you be..\n",
      "после: it makes me sad to see all the bs ppl are writing about and to you two i don't know why they can't let you be\n",
      "\n",
      "----------------------------------------\n",
      "до:  i give up on them !\n",
      "после: i give up on them\n",
      "\n",
      "----------------------------------------\n",
      "до:  @jeanettejoy  Well said.  Never follow someone because of their name, follow because they have something worth listening too\n",
      "после: well said never follow someone because of their name follow because they have something worth listening too\n",
      "\n",
      "----------------------------------------\n",
      "до:  Working on 12 with @sherrisnack\n",
      "после: working on 12 with\n",
      "\n",
      "----------------------------------------\n",
      "после фильтрации по длине [4–16]: 198079 текстов\n",
      "средняя длина: 9.6 слов\n",
      "мин: 4, макс: 16\n",
      "----------------------------------------\n",
      "train: 158463\n",
      "val:   19808\n",
      "test:  19808\n",
      "----------------------------------------\n",
      "размер словаря: 61013\n",
      "примеры: 'love' -> 6870, 'the' -> 22408\n",
      "----------------------------------------\n",
      "создаём датасеты и даталоадеры\n",
      "----------------------------------------\n",
      "размер train_dataset: 1363329 пар (контекст → следующий токен)\n",
      "----------------------------------------\n",
      "пример из train_dataset[0]: input=tensor([    0,     0,     0,     0,     0,     0,     0,     0,     0, 39991]), target=15758\n",
      "----------------------------------------\n",
      "x_batch.shape: torch.Size([128, 10])  # [b, seq_len]\n",
      "y_batch.shape: torch.Size([128])  # [b]\n",
      "----------------------------------------\n",
      "сохраняем артефакты\n",
      "датасет готов, словарь сохранён\n"
     ]
    }
   ],
   "source": [
    "from src.data_utils import (\n",
    "    prepare_data,\n",
    "    create_data_loaders,\n",
    "    save_artifacts\n",
    ")\n",
    "\n",
    "# запуск подготовки\n",
    "train_texts, val_texts, test_texts, word_to_idx, idx_to_word, vocab_size, seq_len = prepare_data(\n",
    "    file_path='data/raw_data.csv',\n",
    "    sample_ratio=0.2,\n",
    "    min_len=4,\n",
    "    max_len=16,\n",
    "    seq_len=10,\n",
    "    random_seed=42\n",
    ")\n",
    "\n",
    "# даталоадеры\n",
    "train_loader, val_loader, test_loader, train_dataset, val_dataset, test_dataset = create_data_loaders(\n",
    "    train_texts, val_texts, test_texts,\n",
    "    word_to_idx=word_to_idx,\n",
    "    seq_len=seq_len,\n",
    "    batch_size=128\n",
    ")\n",
    "\n",
    "# сохраняем\n",
    "save_artifacts(word_to_idx, idx_to_word, vocab_size, seq_len, train_dataset, val_dataset, test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20ed40b",
   "metadata": {},
   "source": [
    "## Этап 2. Объявление модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2e43ab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "LSTMTokenizerModel(\n",
      "  (embedding): Embedding(61013, 128, padding_idx=0)\n",
      "  (lstm): LSTM(128, 256, num_layers=2, batch_first=True)\n",
      "  (fc): Linear(in_features=256, out_features=61013, bias=True)\n",
      ")\n",
      "----------------------------------------\n",
      "размер словаря: 61013\n",
      "модель создана на устройстве: cuda\n",
      "количество параметров: 24,411,605\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import torch\n",
    "from src.lstm_model import LSTMTokenizerModel\n",
    "\n",
    "# словарь\n",
    "with open('data/vocab_final.pkl', 'rb') as f:\n",
    "    vocab_data = pickle.load(f)\n",
    "\n",
    "# данные из словаря\n",
    "word_to_idx = vocab_data['word_to_idx']\n",
    "idx_to_word = vocab_data['idx_to_word']\n",
    "\n",
    "# параметры модели\n",
    "vocab_size = vocab_data['vocab_size']\n",
    "embed_dim = 128\n",
    "hidden_dim = 256\n",
    "num_layers = 2\n",
    "seq_len = vocab_data['seq_len'] \n",
    "\n",
    "# экземпляр модели\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = LSTMTokenizerModel(\n",
    "    vocab_size=vocab_size,\n",
    "    embed_dim=embed_dim,\n",
    "    hidden_dim=hidden_dim,\n",
    "    num_layers=num_layers\n",
    ").to(device)\n",
    "\n",
    "print(\"-\" * 40)\n",
    "print(model)\n",
    "print(\"-\" * 40)\n",
    "print(f\"размер словаря: {vocab_size}\")\n",
    "print(f\"модель создана на устройстве: {device}\")\n",
    "print(f\"количество параметров: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6bc077",
   "metadata": {},
   "source": [
    "## Этап 3. Тренировка модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bcd7842c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "словарь загружен: 61013 токенов\n",
      "пример: 'the' -> 30835\n",
      "длина контекста (seq_len): 10\n",
      "размер обучающей выборки: 1363329\n",
      "пример входа: (tensor([    0,     0,     0,     0,     0,     0,     0,     0,     0, 37377]), tensor(56245)) -> you\n",
      "используем устройство: cuda\n",
      "----------------------------------------\n",
      "\n",
      "дебаг: проверка данных\n",
      "----------------------------------------\n",
      "пример x_batch[0]: [48559, 58931, 58898, 51863, 58931, 58898, 58822, 3918, 15452, 35049]\n",
      "y_batch.min(): 326\n",
      "y_batch.max(): 60806\n",
      "forward и loss работают\n",
      "----------------------------------------\n",
      "фиксированный пример для сравнения каждую эпоху\n",
      "----------------------------------------\n",
      "контекст: thanks\n",
      "реальное продолжение: for\n",
      "ожидаем: thanks for\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1:  22%|██▏       | 2304/10652 [00:29<01:48, 77.13it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meval_lstm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m evaluate_final\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# запуск обучения\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m model, word_to_idx, idx_to_word, seq_len, device, test_loader \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# финальная оценка\u001b[39;00m\n\u001b[0;32m     14\u001b[0m evaluate_final(model, test_loader, word_to_idx, idx_to_word, seq_len, device)\n",
      "File \u001b[1;32mc:\\Users\\Лена\\text-autocomplete-project\\src\\lstm_train.py:110\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    108\u001b[0m x_batch, y_batch \u001b[38;5;241m=\u001b[39m x_batch\u001b[38;5;241m.\u001b[39mto(device), y_batch\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    109\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 110\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    111\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(logits, y_batch)\n\u001b[0;32m    112\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\Лена\\text-autocomplete-project\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Лена\\text-autocomplete-project\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Лена\\text-autocomplete-project\\src\\lstm_model.py:13\u001b[0m, in \u001b[0;36mLSTMTokenizerModel.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     12\u001b[0m     embed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding(x)\n\u001b[1;32m---> 13\u001b[0m     lstm_out, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43membed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(lstm_out[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :])\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m logits\n",
      "File \u001b[1;32mc:\\Users\\Лена\\text-autocomplete-project\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Лена\\text-autocomplete-project\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Лена\\text-autocomplete-project\\venv\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:1123\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m   1120\u001b[0m         hx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[0;32m   1122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1123\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1124\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1126\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1127\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1128\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1129\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1130\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1131\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1132\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_first\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1133\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1134\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1135\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\n\u001b[0;32m   1136\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   1137\u001b[0m         batch_sizes,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1144\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional,\n\u001b[0;32m   1145\u001b[0m     )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "from src.lstm_model import generate\n",
    "from src.lstm_train import train \n",
    "from src.eval_lstm import evaluate_final\n",
    "\n",
    "# запуск обучения\n",
    "model, word_to_idx, idx_to_word, seq_len, device, test_loader = train()\n",
    "\n",
    "# финальная оценка\n",
    "evaluate_final(model, test_loader, word_to_idx, idx_to_word, seq_len, device)\n",
    "\n",
    "# примеры генерации\n",
    "print(\"-\" * 40)\n",
    "print(\"генерация\")\n",
    "print(\"-\" * 40)\n",
    "print('i ->', generate(model, 'i', word_to_idx, idx_to_word, seq_len, temperature=0.8, top_k=10))\n",
    "print('i love ->', generate(model, 'i love', word_to_idx, idx_to_word, seq_len, temperature=0.8, top_k=10))\n",
    "print('today ->', generate(model, 'today', word_to_idx, idx_to_word, seq_len, temperature=0.8, top_k=10))\n",
    "print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c524914c",
   "metadata": {},
   "source": [
    "## Этап 4. Использование предобученного трансформера distilgpt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91bdd40e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "загружаем модель distilgpt2\n",
      "используется GPU\n",
      "загружаем и подготавливаем данные\n",
      "загружено 1600498 твитов\n",
      "очищено текстов: 1600498\n",
      "отфильтровано текстов: 1251724\n",
      "валидационная выборка: 150 текстов\n",
      "----------------------------------------\n",
      "валидация модели\n",
      "----------------------------------------\n",
      "выполняем оценку модели на валидационной выборке\n",
      "\n",
      "----------------------------------------\n",
      "результаты валидации\n",
      "----------------------------------------\n",
      "метрика: ROUGE-L (точность предсказания следующего слова)\n",
      "количество оценок: 99\n",
      "средний ROUGE-L: 0.0976 ± 0.2930\n",
      "медиана ROUGE-L: 0.0000\n",
      "\n",
      "лучшие примеры предсказаний:\n",
      "----------------------------------------\n",
      "пример 1:\n",
      "контекст:    'what tragedy and disaster in'\n",
      "истинное:    'the'\n",
      "предсказанное: 'the'\n",
      "ROUGE-L:     1.0000\n",
      "\n",
      "пример 2:\n",
      "контекст:    'i miss him can't wait'\n",
      "истинное:    'to'\n",
      "предсказанное: 'to'\n",
      "ROUGE-L:     1.0000\n",
      "\n",
      "пример 3:\n",
      "контекст:    'has lost his ring it'\n",
      "истинное:    's'\n",
      "предсказанное: '’s'\n",
      "ROUGE-L:     1.0000\n",
      "\n",
      "пример 4:\n",
      "контекст:    'is fucked to go back'\n",
      "истинное:    'to'\n",
      "предсказанное: 'to'\n",
      "ROUGE-L:     1.0000\n",
      "\n",
      "пример 5:\n",
      "контекст:    'i m here friend and'\n",
      "истинное:    'i'\n",
      "предсказанное: 'I'\n",
      "ROUGE-L:     1.0000\n",
      "\n",
      "----------------------------------------\n",
      "генерация текстов\n",
      "----------------------------------------\n",
      "генерируем тексты на основе промптов\n",
      "----------------------------------------\n",
      "промпт 1:\n",
      "  вход:  i dived many times for the ball managed to save 50 the\n",
      "  выход: match.\n",
      "\n",
      "  полностью: i dived many times for the ball managed to save 50 the match.\n",
      "\n",
      "----------------------------------------\n",
      "промпт 2:\n",
      "  вход:  i just re pierced\n",
      "  выход: It’s all pretty awesome, but I don't think the real thing is how\n",
      "\n",
      "  полностью: i just re pierced.\n",
      "\n",
      "----------------------------------------\n",
      "промпт 3:\n",
      "  вход:  i couldn't bear to watch it and i thought the\n",
      "  выход: story was about a girl who lost her virginity.\n",
      "\n",
      "  полностью: i couldn't bear to watch it and i thought the story was about a girl who lost her virginity.\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "генерация завершена\n"
     ]
    }
   ],
   "source": [
    "from src.eval_transformer_pipeline import TransformerModel\n",
    "\n",
    "# экземпляр модели\n",
    "model = TransformerModel()\n",
    "\n",
    "# данные\n",
    "test_texts, val_texts = model.prepare_data('data/raw_data.csv')\n",
    "\n",
    "# валидация модели\n",
    "avg_rouge, examples = model.validate_model(val_texts)\n",
    "\n",
    "# генерируем тексты\n",
    "generation_results = model.generate_texts(test_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fcb782",
   "metadata": {},
   "source": [
    "## Этап 5. Формулирование выводов"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8a4b8d",
   "metadata": {},
   "source": [
    "\n",
    "> **Выводы:** Не могу дать однозначный ответ, что лучше. \n",
    "\n",
    "##### Начнём с метрик\n",
    "- Основная метрика по ТЗ, ROUGE, практически бесполезна для оценки distilgpt2, так как эта модель генерирует принципиально новые тексты, а не копирует фразы из эталона (и мы буквально берём из коробки готовый пайплайн, не проводя дообучение на своих данных). \n",
    "Для LSTM, которая часто работает ближе к шаблонному предсказанию, ROUGE ещё может иметь некоторый смысл.\n",
    "- Что касается других метрик, таких как accuracy или perplexity, их использование также некорректно для сравнения, так как мы опять же берём \"из коробки\" distilgpt2, нам нам ни логиты, ни вероятности не дадут \n",
    "(Измерить вроде можно, но скорее бессмысленно (метрики accuracy и perplexity предназначены для оценки языковых моделей на задачах предсказания следующего слова и плохо подходят для оценки качества генерации текста), чем сложно + вручную уже запускала distilgpt2, слишком запарно для данной задачи)\n",
    "\n",
    "##### Основной акцент в сравнении приходится делать на анализ примеров генерации и технические характеристики\n",
    "Поэтому, выбираем:\n",
    "- LSTM, если приоритетом является вычислительная эффективность модели, а не качество текста (жёсткие аппаратные ограничения, например, чат-бот с очень узкой тематикой, мне представляется мобильная игра, в которой чат-бот должен следовать только своему лору (LOR'у, ограниченному набору правил и контекста) и словарь не особо большой).\n",
    "- distilgpt2, если приоритетом является связность (человекочитаемость) и разнообразие генерируемого текста (диалоговые системы, автоматическое дополнение текста, вывод инструкций по узкой тематике, чат-бот для работников компании, не крупной, при больших данных те же x5 используют не лёгкие модели, а берут API OpenAI, например).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8df00da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "загружаем LSTM модель\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Лена\\AppData\\Local\\Temp\\ipykernel_16284\\1950219580.py:17: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  lstm_model.load_state_dict(torch.load('models/best_lstm_model_test.pt', map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "загружаем distilgpt2 модель\n",
      "загружаем модель distilgpt2\n",
      "используется GPU\n",
      "----------------------------------------\n",
      "сравнение генераций\n",
      "----------------------------------------\n",
      "\n",
      "пример 1:\n",
      "промпт: 'i love to play with my'\n",
      "----------------------------------------\n",
      "lstm:      i love to play with my telefono weeeeks flask annoyance funk sheesh inflatables she brum n2 lifeofanurbanninja swimsuits hallmark eggcrate aid\n",
      "distilgpt2: i love to play with my son.\n",
      "\n",
      "\n",
      "The whole thing is pretty much over. I'm really excited to be playing\n",
      "----------------------------------------\n",
      "\n",
      "пример 2:\n",
      "промпт: 'today is a beautiful day for'\n",
      "----------------------------------------\n",
      "lstm:      today is a beautiful day for hahahahahhaha kayley wondow luxury gigandet buble komplimente grouch waterr footballpractice htown customer bizarre roblox daaa\n",
      "distilgpt2: today is a beautiful day for women. We’re excited to be making this day a reality. We hope to see more\n",
      "----------------------------------------\n",
      "\n",
      "пример 3:\n",
      "промпт: 't can't believe how fast time'\n",
      "----------------------------------------\n",
      "lstm:      t can't believe how fast time beckyyy goat reruns congratulate naucit kidding hilburn nut hopkirk footballpractice asn bizarre tru crrrrrazy sshatfield\n",
      "distilgpt2: t can't believe how fast time is going on.\n",
      "\n",
      "\n",
      "\n",
      "The first thing I want to say is that, with this\n",
      "----------------------------------------\n",
      "\n",
      "пример 4:\n",
      "промпт: 'you should try this amazing'\n",
      "----------------------------------------\n",
      "lstm:      you should try this amazing fow par fartwatch sufficient 3more badlyyyy smacktalk enemy reconnective scalpers dormindo meenah arp meeeeeeesh aaarrrggg walc\n",
      "distilgpt2: you should try this amazing project, and it will be awesome.\n",
      "\n",
      "As soon as I get to the conclusion of this\n",
      "----------------------------------------\n",
      "\n",
      "пример 5:\n",
      "промпт: 'we are going to the'\n",
      "----------------------------------------\n",
      "lstm:      we are going to the congratulate snaply pasty tini bagde ayunda agora probbaly nomnomnom penet honu admirer ronald decreasing creasor congratulate\n",
      "distilgpt2: we are going to the same place they were last year, and I'm excited that we can be back in the business again\n",
      "----------------------------------------\n",
      "\n",
      "пример 6:\n",
      "промпт: 'the weather is really nice'\n",
      "----------------------------------------\n",
      "lstm:      the weather is really nice 187 decreasing maddddddiiii bizarre reeeeeeeally bearly resulting underperformed bargain mobi nashville showtunes 187 joe flask bizarre\n",
      "distilgpt2: the weather is really nice as a tropical storm.\"\n",
      "\n",
      "\n",
      "\n",
      "A tropical storm in the northeast was named after the Virgin\n",
      "----------------------------------------\n",
      "\n",
      "пример 7:\n",
      "промпт: 'i think we should go to'\n",
      "----------------------------------------\n",
      "lstm:      i think we should go to didn 187 wednesdayy duffle snogging eine stinx ppls rhode quedadilla spine daaa stinx didn ronald\n",
      "distilgpt2: i think we should go to war with each other and we should do that,\" he said.\n",
      "----------------------------------------\n",
      "\n",
      "пример 8:\n",
      "промпт: 'this is the best movie I'\n",
      "----------------------------------------\n",
      "lstm:      this is the best movie I wreaking coldness baah roooomies thehannabeth complimented spools daaa twerps travolta surviving joe daaa nespresso rib\n",
      "distilgpt2: this is the best movie I've ever seen. And if that was the case, I'd say it was the best movie I\n",
      "----------------------------------------\n",
      "\n",
      "пример 9:\n",
      "промпт: 'she always knows how to make'\n",
      "----------------------------------------\n",
      "lstm:      she always knows how to make toc daaa expectin kk flask wankers unfit fead bizarre fead 12 birthdaaaaay dfb lool dawgs\n",
      "distilgpt2: she always knows how to make it happen, so I wanted to give the impression that I was a true friend.\n",
      "\n",
      "It\n",
      "----------------------------------------\n",
      "\n",
      "пример 10:\n",
      "промпт: 'life is too short to'\n",
      "----------------------------------------\n",
      "lstm:      life is too short to betr 11th embarrasing resulting bizarre mocca jangle buonanotte weree cuzn resulting qod congratulate hittin' remembered padma\n",
      "distilgpt2: life is too short to go with that.\n",
      "\n",
      "The second major issue of the show is that the series is so much\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "интерактивная генерация\n",
      "----------------------------------------\n",
      "\n",
      "промпт: 'its the best day of'\n",
      "----------------------------------------\n",
      "lstm:      its the best day of unity ooffoo ringed lunchhh settles coolin tini 2dy daaa yeaaaaaah heheee obedient recco resulting i80 ammon\n",
      "distilgpt2: its the best day of the week.※\n",
      "\n",
      "\n",
      "For more information, visit our “About the Year‬,‭.\n",
      "Like this: Like\n",
      "----------------------------------------\n",
      "\n",
      "завершено!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import pickle\n",
    "from src.lstm_model import LSTMTokenizerModel, generate\n",
    "from src.eval_transformer_pipeline import TransformerModel\n",
    "\n",
    "# загрузка lstm\n",
    "print(\"загружаем LSTM модель\")\n",
    "with open('data/vocab_final.pkl', 'rb') as f:\n",
    "    vocab_data = pickle.load(f)\n",
    "\n",
    "word_to_idx = vocab_data['word_to_idx']\n",
    "idx_to_word = vocab_data['idx_to_word']\n",
    "seq_len = vocab_data['seq_len']\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "lstm_model = LSTMTokenizerModel(len(word_to_idx)).to(device)\n",
    "lstm_model.load_state_dict(torch.load('models/best_lstm_model_test.pt', map_location=device))\n",
    "lstm_model.eval()\n",
    "\n",
    "# загрузка distilgpt2 модели\n",
    "print(\"-\" * 40)\n",
    "print(\"загружаем distilgpt2 модель\")\n",
    "gpt2_model = TransformerModel()\n",
    "gpt2_generator = gpt2_model.generator\n",
    "\n",
    "# 10 примеров для генерации\n",
    "test_examples = [\n",
    "    \"i love to play with my\",\n",
    "    \"today is a beautiful day for\",\n",
    "    \"t can't believe how fast time\",\n",
    "    \"you should try this amazing\",\n",
    "    \"we are going to the\",\n",
    "    \"the weather is really nice\",\n",
    "    \"i think we should go to\",\n",
    "    \"this is the best movie I\",\n",
    "    \"she always knows how to make\",\n",
    "    \"life is too short to\"\n",
    "]\n",
    "\n",
    "print(\"-\" * 40)\n",
    "print(\"сравнение генераций\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# генерация для тестовых примеров\n",
    "for i, prompt in enumerate(test_examples, 1):\n",
    "    print(f\"\\nпример {i}:\")\n",
    "    print(f\"промпт: '{prompt}'\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # генерация lstm\n",
    "    lstm_output = generate(lstm_model, prompt, word_to_idx, idx_to_word, seq_len, max_len=20)\n",
    "    print(f\"lstm:      {lstm_output}\")\n",
    "    \n",
    "    # генерация distilgpt2\n",
    "    try:\n",
    "        gpt2_result = gpt2_generator(\n",
    "            prompt,\n",
    "            max_new_tokens=20,\n",
    "            do_sample=True,\n",
    "            temperature=0.8,\n",
    "            top_p=0.9,\n",
    "            num_return_sequences=1,\n",
    "            pad_token_id=gpt2_generator.tokenizer.eos_token_id\n",
    "        )\n",
    "        gpt2_output = gpt2_result[0]['generated_text'].strip()\n",
    "        print(f\"distilgpt2: {gpt2_output}\")\n",
    "    except Exception as e:\n",
    "        print(f\"distilgpt2: ошибка генерации - {e}\")\n",
    "    \n",
    "    print(\"-\" * 40)\n",
    "\n",
    "# интерактивный ввод промпта\n",
    "print(\"-\" * 40)\n",
    "print(\"интерактивная генерация\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "while True:\n",
    "    user_prompt = input(\"\\nвведите свой промпт (или 'exit' для выхода): \")\n",
    "    \n",
    "    if user_prompt.lower() == 'exit':\n",
    "        break\n",
    "    \n",
    "    if not user_prompt.strip():\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\nпромпт: '{user_prompt}'\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # генерация lstm\n",
    "    try:\n",
    "        lstm_output = generate(lstm_model, user_prompt, word_to_idx, idx_to_word, seq_len, max_len=20)\n",
    "        print(f\"lstm:      {lstm_output}\")\n",
    "    except Exception as e:\n",
    "        print(f\"lstm: ошибка генерации - {e}\")\n",
    "    \n",
    "    # генерация distilgpt2\n",
    "    try:\n",
    "        gpt2_result = gpt2_generator(\n",
    "            user_prompt,\n",
    "            max_new_tokens=30,\n",
    "            do_sample=True,\n",
    "            temperature=0.8,\n",
    "            top_p=0.9,\n",
    "            num_return_sequences=1,\n",
    "            pad_token_id=gpt2_generator.tokenizer.eos_token_id\n",
    "        )\n",
    "        gpt2_output = gpt2_result[0]['generated_text'].strip()\n",
    "        print(f\"distilgpt2: {gpt2_output}\")\n",
    "    except Exception as e:\n",
    "        print(f\"distilgpt2: ошибка генерации - {e}\")\n",
    "    \n",
    "    print(\"-\" * 40)\n",
    "\n",
    "print(\"\\nзавершено\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
