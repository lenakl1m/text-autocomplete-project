{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec40bd00",
   "metadata": {},
   "source": [
    "## Этап 1. Сбор и подготовка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "473da1fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "загружаем и подготавливаем данные\n",
      "----------------------------------------\n",
      "загружено 1600498 строк из файла\n",
      "всего строк в файле: 1600498\n",
      "оставлено после сэмплирования: 320099 строк\n",
      "очищено текстов: 320099\n",
      "----------------------------------------\n",
      "\n",
      "примеры очистки:\n",
      "до:  @bkajino That awesome was for the walk, not the blisters\n",
      "после: that awesome was for the walk not the blisters\n",
      "\n",
      "----------------------------------------\n",
      "до:  @trent_reznor @mariqueen it makes me sad to see all the bs ppl are writing about and to you two.  I don't know why they can't let you be..\n",
      "после: it makes me sad to see all the bs ppl are writing about and to you two i don't know why they can't let you be\n",
      "\n",
      "----------------------------------------\n",
      "до:  i give up on them !\n",
      "после: i give up on them\n",
      "\n",
      "----------------------------------------\n",
      "до:  @jeanettejoy  Well said.  Never follow someone because of their name, follow because they have something worth listening too\n",
      "после: well said never follow someone because of their name follow because they have something worth listening too\n",
      "\n",
      "----------------------------------------\n",
      "до:  Working on 12 with @sherrisnack\n",
      "после: working on 12 with\n",
      "\n",
      "----------------------------------------\n",
      "после фильтрации по длине [4–16]: 198079 текстов\n",
      "средняя длина: 9.6 слов\n",
      "мин: 4, макс: 16\n",
      "----------------------------------------\n",
      "train: 158463\n",
      "val:   19808\n",
      "test:  19808\n",
      "----------------------------------------\n",
      "размер словаря: 61013\n",
      "примеры: 'love' -> 6870, 'the' -> 22408\n",
      "----------------------------------------\n",
      "создаём датасеты и даталоадеры\n",
      "----------------------------------------\n",
      "размер train_dataset: 1363329 пар (контекст → следующий токен)\n",
      "----------------------------------------\n",
      "пример из train_dataset[0]: input=tensor([    0,     0,     0,     0,     0,     0,     0,     0,     0, 39991]), target=15758\n",
      "----------------------------------------\n",
      "x_batch.shape: torch.Size([128, 10])  # [b, seq_len]\n",
      "y_batch.shape: torch.Size([128])  # [b]\n",
      "----------------------------------------\n",
      "сохраняем артефакты\n",
      "датасет готов, словарь сохранён\n"
     ]
    }
   ],
   "source": [
    "from src.data_utils import (\n",
    "    prepare_data,\n",
    "    create_data_loaders,\n",
    "    save_artifacts\n",
    ")\n",
    "\n",
    "# запуск подготовки\n",
    "train_texts, val_texts, test_texts, word_to_idx, idx_to_word, vocab_size, seq_len = prepare_data(\n",
    "    file_path='data/raw_data.csv',\n",
    "    sample_ratio=0.2,\n",
    "    min_len=4,\n",
    "    max_len=16,\n",
    "    seq_len=10,\n",
    "    random_seed=42\n",
    ")\n",
    "\n",
    "# даталоадеры\n",
    "train_loader, val_loader, test_loader, train_dataset, val_dataset, test_dataset = create_data_loaders(\n",
    "    train_texts, val_texts, test_texts,\n",
    "    word_to_idx=word_to_idx,\n",
    "    seq_len=seq_len,\n",
    "    batch_size=128\n",
    ")\n",
    "\n",
    "# сохраняем\n",
    "save_artifacts(word_to_idx, idx_to_word, vocab_size, seq_len, train_dataset, val_dataset, test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20ed40b",
   "metadata": {},
   "source": [
    "## Этап 2. Объявление модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2e43ab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "LSTMTokenizerModel(\n",
      "  (embedding): Embedding(61013, 128, padding_idx=0)\n",
      "  (lstm): LSTM(128, 256, num_layers=2, batch_first=True)\n",
      "  (fc): Linear(in_features=256, out_features=61013, bias=True)\n",
      ")\n",
      "----------------------------------------\n",
      "размер словаря: 61013\n",
      "модель создана на устройстве: cuda\n",
      "количество параметров: 24,411,605\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import torch\n",
    "from src.lstm_model import LSTMTokenizerModel\n",
    "\n",
    "# словарь\n",
    "with open('data/vocab_final.pkl', 'rb') as f:\n",
    "    vocab_data = pickle.load(f)\n",
    "\n",
    "# данные из словаря\n",
    "word_to_idx = vocab_data['word_to_idx']\n",
    "idx_to_word = vocab_data['idx_to_word']\n",
    "\n",
    "# параметры модели\n",
    "vocab_size = vocab_data['vocab_size']\n",
    "embed_dim = 128\n",
    "hidden_dim = 256\n",
    "num_layers = 2\n",
    "seq_len = vocab_data['seq_len'] \n",
    "\n",
    "# экземпляр модели\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = LSTMTokenizerModel(\n",
    "    vocab_size=vocab_size,\n",
    "    embed_dim=embed_dim,\n",
    "    hidden_dim=hidden_dim,\n",
    "    num_layers=num_layers\n",
    ").to(device)\n",
    "\n",
    "print(\"-\" * 40)\n",
    "print(model)\n",
    "print(\"-\" * 40)\n",
    "print(f\"размер словаря: {vocab_size}\")\n",
    "print(f\"модель создана на устройстве: {device}\")\n",
    "print(f\"количество параметров: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6bc077",
   "metadata": {},
   "source": [
    "## Этап 3. Тренировка модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bcd7842c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "словарь загружен: 61013 токенов\n",
      "пример: 'the' -> 30835\n",
      "длина контекста (seq_len): 10\n",
      "размер обучающей выборки: 1363329\n",
      "пример входа: (tensor([    0,     0,     0,     0,     0,     0,     0,     0,     0, 37377]), tensor(56245)) -> you\n",
      "используем устройство: cuda\n",
      "----------------------------------------\n",
      "\n",
      "дебаг: проверка данных\n",
      "----------------------------------------\n",
      "пример x_batch[0]: [48559, 58931, 58898, 51863, 58931, 58898, 58822, 3918, 15452, 35049]\n",
      "y_batch.min(): 326\n",
      "y_batch.max(): 60806\n",
      "forward и loss работают\n",
      "----------------------------------------\n",
      "фиксированный пример для сравнения каждую эпоху\n",
      "----------------------------------------\n",
      "контекст: thanks\n",
      "реальное продолжение: for\n",
      "ожидаем: thanks for\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1:  22%|██▏       | 2304/10652 [00:29<01:48, 77.13it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meval_lstm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m evaluate_final\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# запуск обучения\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m model, word_to_idx, idx_to_word, seq_len, device, test_loader \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# финальная оценка\u001b[39;00m\n\u001b[0;32m     14\u001b[0m evaluate_final(model, test_loader, word_to_idx, idx_to_word, seq_len, device)\n",
      "File \u001b[1;32mc:\\Users\\Лена\\text-autocomplete-project\\src\\lstm_train.py:110\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    108\u001b[0m x_batch, y_batch \u001b[38;5;241m=\u001b[39m x_batch\u001b[38;5;241m.\u001b[39mto(device), y_batch\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    109\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 110\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    111\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(logits, y_batch)\n\u001b[0;32m    112\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\Лена\\text-autocomplete-project\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Лена\\text-autocomplete-project\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Лена\\text-autocomplete-project\\src\\lstm_model.py:13\u001b[0m, in \u001b[0;36mLSTMTokenizerModel.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     12\u001b[0m     embed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding(x)\n\u001b[1;32m---> 13\u001b[0m     lstm_out, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43membed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(lstm_out[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :])\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m logits\n",
      "File \u001b[1;32mc:\\Users\\Лена\\text-autocomplete-project\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Лена\\text-autocomplete-project\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Лена\\text-autocomplete-project\\venv\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:1123\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m   1120\u001b[0m         hx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[0;32m   1122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1123\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1124\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1126\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1127\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1128\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1129\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1130\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1131\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1132\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_first\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1133\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1134\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1135\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\n\u001b[0;32m   1136\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   1137\u001b[0m         batch_sizes,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1144\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional,\n\u001b[0;32m   1145\u001b[0m     )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "from src.lstm_model import generate\n",
    "from src.lstm_train import train \n",
    "from src.eval_lstm import evaluate_final\n",
    "\n",
    "# запуск обучения\n",
    "model, word_to_idx, idx_to_word, seq_len, device, test_loader = train()\n",
    "\n",
    "# финальная оценка\n",
    "evaluate_final(model, test_loader, word_to_idx, idx_to_word, seq_len, device)\n",
    "\n",
    "# примеры генерации\n",
    "print(\"-\" * 40)\n",
    "print(\"генерация\")\n",
    "print(\"-\" * 40)\n",
    "print('i ->', generate(model, 'i', word_to_idx, idx_to_word, seq_len, temperature=0.8, top_k=10))\n",
    "print('i love ->', generate(model, 'i love', word_to_idx, idx_to_word, seq_len, temperature=0.8, top_k=10))\n",
    "print('today ->', generate(model, 'today', word_to_idx, idx_to_word, seq_len, temperature=0.8, top_k=10))\n",
    "print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c524914c",
   "metadata": {},
   "source": [
    "## Этап 4. Использование предобученного трансформера distilgpt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91bdd40e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "загружаем модель distilgpt2\n",
      "используется GPU\n",
      "загружаем и подготавливаем данные\n",
      "загружено 1600498 твитов\n",
      "очищено текстов: 1600498\n",
      "отфильтровано текстов: 1251724\n",
      "валидационная выборка: 150 текстов\n",
      "----------------------------------------\n",
      "валидация модели\n",
      "----------------------------------------\n",
      "выполняем оценку модели на валидационной выборке\n",
      "\n",
      "----------------------------------------\n",
      "результаты валидации\n",
      "----------------------------------------\n",
      "метрика: ROUGE-L (точность предсказания следующего слова)\n",
      "количество оценок: 99\n",
      "средний ROUGE-L: 0.0976 ± 0.2930\n",
      "медиана ROUGE-L: 0.0000\n",
      "\n",
      "лучшие примеры предсказаний:\n",
      "----------------------------------------\n",
      "пример 1:\n",
      "контекст:    'what tragedy and disaster in'\n",
      "истинное:    'the'\n",
      "предсказанное: 'the'\n",
      "ROUGE-L:     1.0000\n",
      "\n",
      "пример 2:\n",
      "контекст:    'i miss him can't wait'\n",
      "истинное:    'to'\n",
      "предсказанное: 'to'\n",
      "ROUGE-L:     1.0000\n",
      "\n",
      "пример 3:\n",
      "контекст:    'has lost his ring it'\n",
      "истинное:    's'\n",
      "предсказанное: '’s'\n",
      "ROUGE-L:     1.0000\n",
      "\n",
      "пример 4:\n",
      "контекст:    'is fucked to go back'\n",
      "истинное:    'to'\n",
      "предсказанное: 'to'\n",
      "ROUGE-L:     1.0000\n",
      "\n",
      "пример 5:\n",
      "контекст:    'i m here friend and'\n",
      "истинное:    'i'\n",
      "предсказанное: 'I'\n",
      "ROUGE-L:     1.0000\n",
      "\n",
      "----------------------------------------\n",
      "генерация текстов\n",
      "----------------------------------------\n",
      "генерируем тексты на основе промптов\n",
      "----------------------------------------\n",
      "промпт 1:\n",
      "  вход:  i dived many times for the ball managed to save 50 the\n",
      "  выход: match.\n",
      "\n",
      "  полностью: i dived many times for the ball managed to save 50 the match.\n",
      "\n",
      "----------------------------------------\n",
      "промпт 2:\n",
      "  вход:  i just re pierced\n",
      "  выход: It’s all pretty awesome, but I don't think the real thing is how\n",
      "\n",
      "  полностью: i just re pierced.\n",
      "\n",
      "----------------------------------------\n",
      "промпт 3:\n",
      "  вход:  i couldn't bear to watch it and i thought the\n",
      "  выход: story was about a girl who lost her virginity.\n",
      "\n",
      "  полностью: i couldn't bear to watch it and i thought the story was about a girl who lost her virginity.\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "генерация завершена\n"
     ]
    }
   ],
   "source": [
    "from src.eval_transformer_pipeline import TransformerModel\n",
    "\n",
    "# экземпляр модели\n",
    "model = TransformerModel()\n",
    "\n",
    "# данные\n",
    "test_texts, val_texts = model.prepare_data('data/raw_data.csv')\n",
    "\n",
    "# валидация модели\n",
    "avg_rouge, examples = model.validate_model(val_texts)\n",
    "\n",
    "# генерируем тексты\n",
    "generation_results = model.generate_texts(test_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fcb782",
   "metadata": {},
   "source": [
    "## Этап 5. Формулирование выводов"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8a4b8d",
   "metadata": {},
   "source": [
    "\n",
    "> **Выводы:** Не могу дать однозначный ответ, что лучше. \n",
    "\n",
    "##### Начнём с метрик\n",
    "- Основная метрика по ТЗ, ROUGE, практически бесполезна для оценки distilgpt2, так как эта модель генерирует принципиально новые тексты, а не копирует фразы из эталона (и мы буквально берём из коробки готовый пайплайн, не проводя дообучение на своих данных). \n",
    "Для LSTM, которая часто работает ближе к шаблонному предсказанию, ROUGE ещё может иметь некоторый смысл.\n",
    "- Что касается других метрик, таких как accuracy или perplexity, их использование также некорректно для сравнения, так как мы опять же берём \"из коробки\" distilgpt2, нам нам ни логиты, ни вероятности не дадут \n",
    "(Измерить вроде можно, но скорее бессмысленно (метрики accuracy и perplexity предназначены для оценки языковых моделей на задачах предсказания следующего слова и плохо подходят для оценки качества генерации текста), чем сложно + вручную уже запускала distilgpt2, слишком запарно для данной задачи)\n",
    "\n",
    "##### Основной акцент в сравнении приходится делать на анализ примеров генерации и технические характеристики\n",
    "Поэтому, выбираем:\n",
    "- LSTM, если приоритетом является вычислительная эффективность модели, а не качество текста (жёсткие аппаратные ограничения, например, чат-бот с очень узкой тематикой, мне представляется мобильная игра, в которой чат-бот должен следовать только своему лору (LOR'у, ограниченному набору правил и контекста) и словарь не особо большой).\n",
    "- distilgpt2, если приоритетом является связность (человекочитаемость) и разнообразие генерируемого текста (диалоговые системы, автоматическое дополнение текста, вывод инструкций по узкой тематике, чат-бот для работников компании, не крупной, при больших данных те же x5 используют не лёгкие модели, а берут API OpenAI, например).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8df00da",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
